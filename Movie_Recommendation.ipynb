{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Movie Recommendation",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/machtg/movie-recommendation-bangkit/blob/master/Movie_Recommendation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03SQvSoV-crG",
        "colab_type": "text"
      },
      "source": [
        "# Movie Recommendation\n",
        "\n",
        "by Hanjaya Suryalim and Maclaurin Hutagalung\n",
        "\n",
        "Data was taken from [Kaggle dataset](https://www.kaggle.com/zeeshanmulla/recommendation-system-movie). The data consists of movies ratings (on a scale of 1 to 5).\n",
        "\n",
        "Most of the code being used here are taken from [Recommendation Systems course by Google](https://colab.research.google.com/github/google/eng-edu/blob/master/ml/recommendation-systems/recommendation-systems.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqAh0EguEFcU",
        "colab_type": "text"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Let's get started by importing the required packages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyLqBQy5NJ_x",
        "colab_type": "text"
      },
      "source": [
        "## Import Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOliIU7y-il6",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "fa5c8880-5cf5-4e21-cfbe-4e67f7788227",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "source": [
        "# @title Imports (run this cell)\n",
        "from __future__ import print_function\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import collections\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from IPython import display\n",
        "from matplotlib import pyplot as plt\n",
        "import sklearn\n",
        "import sklearn.manifold\n",
        "import tensorflow as tf \n",
        "print(tf.__version__)\n",
        "#import tensorflow as tf\n",
        "#tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "# Add some convenience functions to Pandas DataFrame.\n",
        "pd.options.display.max_rows = 10\n",
        "pd.options.display.float_format = '{:.3f}'.format\n",
        "def mask(df, key, function):\n",
        "  \"\"\"Returns a filtered dataframe, by applying function to key\"\"\"\n",
        "  return df[function(df[key])]\n",
        "\n",
        "def flatten_cols(df):\n",
        "  df.columns = [' '.join(col).strip() for col in df.columns.values]\n",
        "  return df\n",
        "\n",
        "pd.DataFrame.mask = mask\n",
        "pd.DataFrame.flatten_cols = flatten_cols\n",
        "\n",
        "# Install Altair and activate its colab renderer.\n",
        "print(\"Installing Altair...\")\n",
        "!pip install git+git://github.com/altair-viz/altair.git\n",
        "import altair as alt\n",
        "alt.data_transformers.enable('default', max_rows=None)\n",
        "alt.renderers.enable('colab')\n",
        "print(\"Done installing Altair.\")\n",
        "\n",
        "# Install spreadsheets and import authentication module.\n",
        "USER_RATINGS = False\n",
        "!pip install --upgrade -q gspread\n",
        "from google.colab import auth\n",
        "import gspread\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0-rc2\n",
            "Installing Altair...\n",
            "Collecting git+git://github.com/altair-viz/altair.git\n",
            "  Cloning git://github.com/altair-viz/altair.git to /tmp/pip-req-build-lp0p5_ed\n",
            "  Running command git clone -q git://github.com/altair-viz/altair.git /tmp/pip-req-build-lp0p5_ed\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied (use --upgrade to upgrade): altair==4.2.0.dev0 from git+git://github.com/altair-viz/altair.git in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from altair==4.2.0.dev0) (1.18.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from altair==4.2.0.dev0) (2.11.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.6/dist-packages (from altair==4.2.0.dev0) (2.6.0)\n",
            "Requirement already satisfied: pandas>=0.18 in /usr/local/lib/python3.6/dist-packages (from altair==4.2.0.dev0) (1.0.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.6/dist-packages (from altair==4.2.0.dev0) (0.10.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.6/dist-packages (from altair==4.2.0.dev0) (0.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->altair==4.2.0.dev0) (1.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.18->altair==4.2.0.dev0) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.18->altair==4.2.0.dev0) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas>=0.18->altair==4.2.0.dev0) (1.12.0)\n",
            "Building wheels for collected packages: altair\n",
            "  Building wheel for altair (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for altair: filename=altair-4.2.0.dev0-cp36-none-any.whl size=728122 sha256=ebc1d73900d30ec74e409346bb134bedec89ff8786d9f38d2715d220ba4dc629\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-4q3s_0h2/wheels/01/fd/91/025b6149b3949af76e93b3b3ceca5bf12cbdebc98fa46f9ec6\n",
            "Successfully built altair\n",
            "Done installing Altair.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CREqqCnUNRnA",
        "colab_type": "text"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdOphznfR9KO",
        "colab_type": "code",
        "outputId": "4baa71ce-a0d6-4f30-f83c-d733dfa06c4e",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 79
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9aae2588-9c8c-4d24-a0e5-af84d763897f\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-9aae2588-9c8c-4d24-a0e5-af84d763897f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving Recommendation System.csv to Recommendation System.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mKTDYbEElCf",
        "colab_type": "text"
      },
      "source": [
        "# I. Exploring the Data\n",
        "Before we dive into model building, let's inspect our MovieLens dataset. It is usually helpful to understand the statistics of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7IsQ2JLHYld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "users = pd.read_csv('Recommendation System.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gX-hd6EHp7I",
        "colab_type": "code",
        "outputId": "c15970ca-231c-4996-bbdf-42aeaf46286c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "users.columns=['user_id','item_id','rating','timestamp']\n",
        "\n",
        "# @title Load the MovieLens data (run this cell).\n",
        "# Download MovieLens data.\n",
        "print(\"Downloading movielens data...\")\n",
        "from urllib.request import urlretrieve\n",
        "import zipfile\n",
        "urlretrieve(\"http://files.grouplens.org/datasets/movielens/ml-100k.zip\", \"movielens.zip\")\n",
        "zip_ref = zipfile.ZipFile('movielens.zip', \"r\")\n",
        "zip_ref.extractall()\n",
        "print(\"Done. Dataset contains:\")\n",
        "print(zip_ref.read('ml-100k/u.info'))\n",
        "# Load each data set (users, movies, and ratings).\n",
        "users_cols = ['user_id', 'age', 'sex', 'occupation', 'zip_code']\n",
        "users = pd.read_csv(\n",
        "    'ml-100k/u.user', sep='|', names=users_cols, encoding='latin-1')\n",
        "ratings_cols = ['user_id', 'movie_id', 'rating', 'unix_timestamp']\n",
        "ratings = pd.read_csv(\n",
        "    'ml-100k/u.data', sep='\\t', names=ratings_cols, encoding='latin-1')\n",
        "# The movies file contains a binary feature for each genre.\n",
        "genre_cols = [\n",
        "    \"genre_unknown\", \"Action\", \"Adventure\", \"Animation\", \"Children\", \"Comedy\",\n",
        "    \"Crime\", \"Documentary\", \"Drama\", \"Fantasy\", \"Film-Noir\", \"Horror\",\n",
        "    \"Musical\", \"Mystery\", \"Romance\", \"Sci-Fi\", \"Thriller\", \"War\", \"Western\"\n",
        "]\n",
        "movies_cols = [\n",
        "    'movie_id', 'title', 'release_date', \"video_release_date\", \"imdb_url\"\n",
        "] + genre_cols\n",
        "movies = pd.read_csv(\n",
        "    'ml-100k/u.item', sep='|', names=movies_cols, encoding='latin-1')\n",
        "# Since the ids start at 1, we shift them to start at 0.\n",
        "users[\"user_id\"] = users[\"user_id\"].apply(lambda x: str(x-1))\n",
        "movies[\"movie_id\"] = movies[\"movie_id\"].apply(lambda x: str(x-1))\n",
        "movies[\"year\"] = movies['release_date'].apply(lambda x: str(x).split('-')[-1])\n",
        "ratings[\"movie_id\"] = ratings[\"movie_id\"].apply(lambda x: str(x-1))\n",
        "ratings[\"user_id\"] = ratings[\"user_id\"].apply(lambda x: str(x-1))\n",
        "ratings[\"rating\"] = ratings[\"rating\"].apply(lambda x: float(x))\n",
        "# Compute the number of movies to which a genre is assigned.\n",
        "genre_occurences = movies[genre_cols].sum().to_dict()\n",
        "# Since some movies can belong to more than one genre, we create different\n",
        "# 'genre' columns as follows:\n",
        "# - all_genres: all the active genres of the movie.\n",
        "# - genre: randomly sampled from the active genres.\n",
        "def mark_genres(movies, genres):\n",
        "  def get_random_genre(gs):\n",
        "    active = [genre for genre, g in zip(genres, gs) if g==1]\n",
        "    if len(active) == 0:\n",
        "      return 'Other'\n",
        "    return np.random.choice(active)\n",
        "  def get_all_genres(gs):\n",
        "    active = [genre for genre, g in zip(genres, gs) if g==1]\n",
        "    if len(active) == 0:\n",
        "      return 'Other'\n",
        "    return '-'.join(active)\n",
        "  movies['genre'] = [\n",
        "      get_random_genre(gs) for gs in zip(*[movies[genre] for genre in genres])]\n",
        "  movies['all_genres'] = [\n",
        "      get_all_genres(gs) for gs in zip(*[movies[genre] for genre in genres])]\n",
        "mark_genres(movies, genre_cols)\n",
        "# Create one merged DataFrame containing all the movielens data.\n",
        "movielens = ratings.merge(movies, on='movie_id').merge(users, on='user_id')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading movielens data...\n",
            "Done. Dataset contains:\n",
            "b'943 users\\n1682 items\\n100000 ratings\\n'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2ruKSgDHdTQ",
        "colab_type": "code",
        "outputId": "ec62e600-bb35-4831-c8d2-1f5054a1dd9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "users.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>occupation</th>\n",
              "      <th>zip_code</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>24</td>\n",
              "      <td>M</td>\n",
              "      <td>technician</td>\n",
              "      <td>85711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>53</td>\n",
              "      <td>F</td>\n",
              "      <td>other</td>\n",
              "      <td>94043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>23</td>\n",
              "      <td>M</td>\n",
              "      <td>writer</td>\n",
              "      <td>32067</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>24</td>\n",
              "      <td>M</td>\n",
              "      <td>technician</td>\n",
              "      <td>43537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>33</td>\n",
              "      <td>F</td>\n",
              "      <td>other</td>\n",
              "      <td>15213</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  user_id  age sex  occupation zip_code\n",
              "0       0   24   M  technician    85711\n",
              "1       1   53   F       other    94043\n",
              "2       2   23   M      writer    32067\n",
              "3       3   24   M  technician    43537\n",
              "4       4   33   F       other    15213"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJLkqpy4HeMQ",
        "colab_type": "code",
        "outputId": "6f2a7ccc-5615-446c-9d28-6829c372d3f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "users.tail()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>item_id</th>\n",
              "      <th>rating</th>\n",
              "      <th>timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>99997</th>\n",
              "      <td>880</td>\n",
              "      <td>476</td>\n",
              "      <td>3</td>\n",
              "      <td>880175444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99998</th>\n",
              "      <td>716</td>\n",
              "      <td>204</td>\n",
              "      <td>5</td>\n",
              "      <td>879795543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99999</th>\n",
              "      <td>276</td>\n",
              "      <td>1090</td>\n",
              "      <td>1</td>\n",
              "      <td>874795795</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100000</th>\n",
              "      <td>13</td>\n",
              "      <td>225</td>\n",
              "      <td>2</td>\n",
              "      <td>882399156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100001</th>\n",
              "      <td>12</td>\n",
              "      <td>203</td>\n",
              "      <td>3</td>\n",
              "      <td>879959583</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        user_id  item_id  rating  timestamp\n",
              "99997       880      476       3  880175444\n",
              "99998       716      204       5  879795543\n",
              "99999       276     1090       1  874795795\n",
              "100000       13      225       2  882399156\n",
              "100001       12      203       3  879959583"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuTj6D0DFA93",
        "colab_type": "text"
      },
      "source": [
        "### Users\n",
        "We start by printing some basic statistics describing the numeric user features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2KN7xmAKzYm",
        "colab_type": "code",
        "outputId": "c6270634-f582-41bb-b6ad-bda203da1007",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "users.describe()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>item_id</th>\n",
              "      <th>rating</th>\n",
              "      <th>timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>100002.000</td>\n",
              "      <td>100002.000</td>\n",
              "      <td>100002.000</td>\n",
              "      <td>100002.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>462.476</td>\n",
              "      <td>425.525</td>\n",
              "      <td>3.530</td>\n",
              "      <td>883528805.931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>266.620</td>\n",
              "      <td>330.797</td>\n",
              "      <td>1.126</td>\n",
              "      <td>5343812.461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>874724710.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>254.000</td>\n",
              "      <td>175.000</td>\n",
              "      <td>3.000</td>\n",
              "      <td>879448715.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>447.000</td>\n",
              "      <td>322.000</td>\n",
              "      <td>4.000</td>\n",
              "      <td>882826944.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>682.000</td>\n",
              "      <td>631.000</td>\n",
              "      <td>4.000</td>\n",
              "      <td>888259984.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>943.000</td>\n",
              "      <td>1682.000</td>\n",
              "      <td>5.000</td>\n",
              "      <td>893286638.000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         user_id    item_id     rating     timestamp\n",
              "count 100002.000 100002.000 100002.000    100002.000\n",
              "mean     462.476    425.525      3.530 883528805.931\n",
              "std      266.620    330.797      1.126   5343812.461\n",
              "min        0.000      1.000      1.000 874724710.000\n",
              "25%      254.000    175.000      3.000 879448715.000\n",
              "50%      447.000    322.000      4.000 882826944.000\n",
              "75%      682.000    631.000      4.000 888259984.000\n",
              "max      943.000   1682.000      5.000 893286638.000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3Bs7iALVpW0",
        "colab_type": "text"
      },
      "source": [
        "## Data Quality Assesment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Kll87-rVvLs",
        "colab_type": "text"
      },
      "source": [
        "### Outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybe86Te_W1n8",
        "colab_type": "text"
      },
      "source": [
        "Rating"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KfnwyPUXM5F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pjVpTwJzkIF",
        "colab_type": "text"
      },
      "source": [
        "# II. Preliminaries\n",
        "\n",
        "Our goal is to factorize the ratings matrix $A$ into the product of a user embedding matrix $U$ and movie embedding matrix $V$, such that $A \\approx UV^\\top$ with\n",
        "$U = \\begin{bmatrix} u_{1} \\\\ \\hline \\vdots \\\\ \\hline u_{N} \\end{bmatrix}$ and\n",
        "$V = \\begin{bmatrix} v_{1} \\\\ \\hline \\vdots \\\\ \\hline v_{M} \\end{bmatrix}$.\n",
        "\n",
        "Here\n",
        "- $N$ is the number of users,\n",
        "- $M$ is the number of movies,\n",
        "- $A_{ij}$ is the rating of the $j$th movies by the $i$th user,\n",
        "- each row $U_i$ is a $d$-dimensional vector (embedding) representing user $i$,\n",
        "- each rwo $V_j$ is a $d$-dimensional vector (embedding) representing movie $j$,\n",
        "- the prediction of the model for the $(i, j)$ pair is the dot product $\\langle U_i, V_j \\rangle$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZjS5YJG07jB",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 1: Build a tf.SparseTensor representation of the Rating Matrix.\n",
        "\n",
        "In this exercise, we'll write a function that maps from our `ratings` DataFrame to a `tf.SparseTensor`.\n",
        "\n",
        "Hint: you can select the values of a given column of a Dataframe `df` using `df['column_name'].values`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQG-LvGE1Gbr",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Solution\n",
        "def build_rating_sparse_tensor(ratings_df):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    ratings_df: a pd.DataFrame with `user_id`, `movie_id` and `rating` columns.\n",
        "  Returns:\n",
        "    a tf.SparseTensor representing the ratings matrix.\n",
        "  \"\"\"\n",
        "  indices = ratings_df[['user_id', 'item_id']].values\n",
        "  values = ratings_df['rating'].values\n",
        "  return tf.SparseTensor(\n",
        "      indices=indices,\n",
        "      values=values,\n",
        "      dense_shape=[users.shape[0], movies.shape[0]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbZO7kT41bJa",
        "colab_type": "text"
      },
      "source": [
        "## Calculating the error\n",
        "\n",
        "The model approximates the ratings matrix $A$ by a low-rank product $UV^\\top$. We need a way to measure the approximation error. We'll start by using the Mean Squared Error of observed entries only (we will revisit this later). It is defined as\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\text{MSE}(A, UV^\\top)\n",
        "&= \\frac{1}{|\\Omega|}\\sum_{(i, j) \\in\\Omega}{( A_{ij} - (UV^\\top)_{ij})^2} \\\\\n",
        "&= \\frac{1}{|\\Omega|}\\sum_{(i, j) \\in\\Omega}{( A_{ij} - \\langle U_i, V_j\\rangle)^2}\n",
        "\\end{align*}\n",
        "$$\n",
        "where $\\Omega$ is the set of observed ratings, and $|\\Omega|$ is the cardinality of $\\Omega$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFzw_BDO1-OI",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 2: Mean Squared Error\n",
        "\n",
        "Write a TensorFlow function that takes a sparse rating matrix $A$ and the two embedding matrices $U, V$ and returns the mean squared error $\\text{MSE}(A, UV^\\top)$.\n",
        "\n",
        "Hints:\n",
        "  * in this section, we only consider observed entries when calculating the loss.\n",
        "  * a `SparseTensor` `sp_x` is a tuple of three Tensors: `sp_x.indices`, `sp_x.values` and `sp_x.dense_shape`.\n",
        "  * you may find [`tf.gather_nd`](https://www.tensorflow.org/api_docs/python/tf/gather_nd) and  [`tf.losses.mean_squared_error`](https://www.tensorflow.org/api_docs/python/tf/losses/mean_squared_error) helpful."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHSBzb1O2gdF",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Alternate Solution\n",
        "def sparse_mean_square_error(sparse_ratings, user_embeddings, movie_embeddings):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    sparse_ratings: A SparseTensor rating matrix, of dense_shape [N, M]\n",
        "    user_embeddings: A dense Tensor U of shape [N, k] where k is the embedding\n",
        "      dimension, such that U_i is the embedding of user i.\n",
        "    movie_embeddings: A dense Tensor V of shape [M, k] where k is the embedding\n",
        "      dimension, such that V_j is the embedding of movie j.\n",
        "  Returns:\n",
        "    A scalar Tensor representing the MSE between the true ratings and the\n",
        "      model's predictions.\n",
        "  \"\"\"\n",
        "  predictions = tf.reduce_sum(\n",
        "      tf.gather(user_embeddings, sparse_ratings.indices[:, 0]) *\n",
        "      tf.gather(movie_embeddings, sparse_ratings.indices[:, 1]),\n",
        "      axis=1)\n",
        "  loss = tf.losses.mean_squared_error(sparse_ratings.values, predictions)\n",
        "  return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clJCp1dD5GoH",
        "colab_type": "text"
      },
      "source": [
        "# III. Training a Matrix Factorization model\n",
        "\n",
        "## CFModel (Collaborative Filtering Model) helper class\n",
        "This is a simple class to train a matrix factorization model using stochastic gradient descent.\n",
        "\n",
        "The class constructor takes\n",
        "- the user embeddings U (a `tf.Variable`).\n",
        "- the movie embeddings V, (a `tf.Variable`).\n",
        "- a loss to optimize (a `tf.Tensor`).\n",
        "- an optional list of metrics dictionaries, each mapping a string (the name of the metric) to a tensor. These are evaluated and plotted during training (e.g. training error and test error).\n",
        "\n",
        "After training, one can access the trained embeddings using the `model.embeddings` dictionary.\n",
        "\n",
        "Example usage:\n",
        "```\n",
        "U_var = ...\n",
        "V_var = ...\n",
        "loss = ...\n",
        "model = CFModel(U_var, V_var, loss)\n",
        "model.train(iterations=100, learning_rate=1.0)\n",
        "user_embeddings = model.embeddings['user_id']\n",
        "movie_embeddings = model.embeddings['movie_id']\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6uRBpPM5ZoO",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "# @title CFModel helper class (run this cell)\n",
        "class CFModel(object):\n",
        "  \"\"\"Simple class that represents a collaborative filtering model\"\"\"\n",
        "  def __init__(self, embedding_vars, loss, metrics=None):\n",
        "    \"\"\"Initializes a CFModel.\n",
        "    Args:\n",
        "      embedding_vars: A dictionary of tf.Variables.\n",
        "      loss: A float Tensor. The loss to optimize.\n",
        "      metrics: optional list of dictionaries of Tensors. The metrics in each\n",
        "        dictionary will be plotted in a separate figure during training.\n",
        "    \"\"\"\n",
        "    self._embedding_vars = embedding_vars\n",
        "    self._loss = loss\n",
        "    self._metrics = metrics\n",
        "    self._embeddings = {k: None for k in embedding_vars}\n",
        "    self._session = None\n",
        "\n",
        "  @property\n",
        "  def embeddings(self):\n",
        "    \"\"\"The embeddings dictionary.\"\"\"\n",
        "    return self._embeddings\n",
        "\n",
        "  def train(self, num_iterations=100, learning_rate=1.0, plot_results=True,\n",
        "            optimizer=tf.compat.v1.train.GradientDescentOptimizer):\n",
        "    \"\"\"Trains the model.\n",
        "    Args:\n",
        "      iterations: number of iterations to run.\n",
        "      learning_rate: optimizer learning rate.\n",
        "      plot_results: whether to plot the results at the end of training.\n",
        "      optimizer: the optimizer to use. Default to GradientDescentOptimizer.\n",
        "    Returns:\n",
        "      The metrics dictionary evaluated at the last iteration.\n",
        "    \"\"\"\n",
        "    with self._loss.graph.as_default():\n",
        "      opt = optimizer(learning_rate)\n",
        "      train_op = opt.minimize(self._loss)\n",
        "      local_init_op = tf.group(\n",
        "          tf.variables_initializer(opt.variables()),\n",
        "          tf.local_variables_initializer())\n",
        "      if self._session is None:\n",
        "        self._session = tf.Session()\n",
        "        with self._session.as_default():\n",
        "          self._session.run(tf.global_variables_initializer())\n",
        "          self._session.run(tf.tables_initializer())\n",
        "          tf.train.start_queue_runners()\n",
        "\n",
        "    with self._session.as_default():\n",
        "      local_init_op.run()\n",
        "      iterations = []\n",
        "      metrics = self._metrics or ({},)\n",
        "      metrics_vals = [collections.defaultdict(list) for _ in self._metrics]\n",
        "\n",
        "      # Train and append results.\n",
        "      for i in range(num_iterations + 1):\n",
        "        _, results = self._session.run((train_op, metrics))\n",
        "        if (i % 10 == 0) or i == num_iterations:\n",
        "          print(\"\\r iteration %d: \" % i + \", \".join(\n",
        "                [\"%s=%f\" % (k, v) for r in results for k, v in r.items()]),\n",
        "                end='')\n",
        "          iterations.append(i)\n",
        "          for metric_val, result in zip(metrics_vals, results):\n",
        "            for k, v in result.items():\n",
        "              metric_val[k].append(v)\n",
        "\n",
        "      for k, v in self._embedding_vars.items():\n",
        "        self._embeddings[k] = v.eval()\n",
        "\n",
        "      if plot_results:\n",
        "        # Plot the metrics.\n",
        "        num_subplots = len(metrics)+1\n",
        "        fig = plt.figure()\n",
        "        fig.set_size_inches(num_subplots*10, 8)\n",
        "        for i, metric_vals in enumerate(metrics_vals):\n",
        "          ax = fig.add_subplot(1, num_subplots, i+1)\n",
        "          for k, v in metric_vals.items():\n",
        "            ax.plot(iterations, v, label=k)\n",
        "          ax.set_xlim([1, num_iterations])\n",
        "          ax.legend()\n",
        "      return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kbR_0bP5jKd",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 4: Build a Matrix Factorization model and train it\n",
        "\n",
        "Using your `sparse_mean_square_error` function, write a function that builds a `CFModel` by creating the embedding variables and the train and test losses."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jacuDqZY5uJ1",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Solution\n",
        "# Utility to split the data into training and test sets.\n",
        "def split_dataframe(df, holdout_fraction=0.1):\n",
        "  \"\"\"Splits a DataFrame into training and test sets.\n",
        "  Args:\n",
        "    df: a dataframe.\n",
        "    holdout_fraction: fraction of dataframe rows to use in the test set.\n",
        "  Returns:\n",
        "    train: dataframe for training\n",
        "    test: dataframe for testing\n",
        "  \"\"\"\n",
        "  test = df.sample(frac=holdout_fraction, replace=False)\n",
        "  train = df[~df.index.isin(test.index)]\n",
        "  return train, test\n",
        "\n",
        "def build_model(ratings, embedding_dim=3, init_stddev=1.):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    ratings: a DataFrame of the ratings\n",
        "    embedding_dim: the dimension of the embedding vectors.\n",
        "    init_stddev: float, the standard deviation of the random initial embeddings.\n",
        "  Returns:\n",
        "    model: a CFModel.\n",
        "  \"\"\"\n",
        "  # Split the ratings DataFrame into train and test.\n",
        "  train_ratings, test_ratings = split_dataframe(ratings)\n",
        "  # SparseTensor representation of the train and test datasets.\n",
        "  A_train = build_rating_sparse_tensor(train_ratings)\n",
        "  A_test = build_rating_sparse_tensor(test_ratings)\n",
        "  # Initialize the embeddings using a normal distribution.\n",
        "  U = tf.Variable(tf.random_normal(\n",
        "      [A_train.dense_shape[0], embedding_dim], stddev=init_stddev))\n",
        "  V = tf.Variable(tf.random_normal(\n",
        "      [A_train.dense_shape[1], embedding_dim], stddev=init_stddev))\n",
        "  train_loss = sparse_mean_square_error(A_train, U, V)\n",
        "  test_loss = sparse_mean_square_error(A_test, U, V)\n",
        "  metrics = {\n",
        "      'train_error': train_loss,\n",
        "      'test_error': test_loss\n",
        "  }\n",
        "  embeddings = {\n",
        "      \"user_id\": U,\n",
        "      \"movie_id\": V\n",
        "  }\n",
        "  return CFModel(embeddings, train_loss, [metrics])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jt7zJEg7562X",
        "colab_type": "text"
      },
      "source": [
        "Great, now it's time to train the model!\n",
        "\n",
        "Go ahead and run the next cell, trying different parameters (embedding dimension, learning rate, iterations). The training and test errors are plotted at the end of training. You can inspect these values to validate the hyper-parameters.\n",
        "\n",
        "Note: by calling `model.train` again, the model will continue training starting from the current values of the embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyEQ3jg75-d0",
        "colab_type": "code",
        "outputId": "be7422e1-0164-42b6-eed7-9e10a78672b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        }
      },
      "source": [
        "# Build the CF model and train it.\n",
        "model = build_model(users, embedding_dim=30, init_stddev=0.5)\n",
        "model.train(num_iterations=1000, learning_rate=10.)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-a05beb43db62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0musers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_stddev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-63-da7af1143f64>\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(ratings, embedding_dim, init_stddev)\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0mtrain_ratings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_ratings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mratings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0;31m# SparseTensor representation of the train and test datasets.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m   \u001b[0mA_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_rating_sparse_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ratings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m   \u001b[0mA_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_rating_sparse_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_ratings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0;31m# Initialize the embeddings using a normal distribution.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-69-d674930d46e4>\u001b[0m in \u001b[0;36mbuild_rating_sparse_tensor\u001b[0;34m(ratings_df)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m \u001b[0mrepresenting\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mratings\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \"\"\"\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mratings_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'user_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'item_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mratings_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rating'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   return tf.SparseTensor(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2804\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2805\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2806\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2808\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         self._validate_read_indexer(\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m         )\n\u001b[1;32m   1555\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1644\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"loc\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1645\u001b[0m                 \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1646\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1648\u001b[0m             \u001b[0;31m# we skip the warning on Categorical/Interval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['item_id'] not in index\""
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPZAcjyT6YN0",
        "colab_type": "text"
      },
      "source": [
        "# IV. Inspecting the Embeddings\n",
        "\n",
        "In this section, we take a closer look at the learned embeddings, by\n",
        "- computing your recommendations\n",
        "- looking at the nearest neighbors of some movies,\n",
        "- looking at the norms of the movie embeddings,\n",
        "- visualizing the embedding in a projected embedding space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lme6Ss3z6dfy",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 5: Write a function that computes the scores of the candidates\n",
        "We start by writing a function that, given a query embedding $u \\in \\mathbb R^d$ and item embeddings $V \\in \\mathbb R^{N \\times d}$, computes the item scores.\n",
        "\n",
        "As discussed in the lecture, there are different similarity measures we can use, and these can yield different results. We will compare the following:\n",
        "- dot product: the score of item j is $\\langle u, V_j \\rangle$.\n",
        "- cosine: the score of item j is $\\frac{\\langle u, V_j \\rangle}{\\|u\\|\\|V_j\\|}$.\n",
        "\n",
        "Hints:\n",
        "- you can use [`np.dot`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html) to compute the product of two np.Arrays.\n",
        "- you can use [`np.linalg.norm`](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.linalg.norm.html) to compute the norm of a np.Array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l20tRwaU69U3",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Solution\n",
        "DOT = 'dot'\n",
        "COSINE = 'cosine'\n",
        "def compute_scores(query_embedding, item_embeddings, measure=DOT):\n",
        "  \"\"\"Computes the scores of the candidates given a query.\n",
        "  Args:\n",
        "    query_embedding: a vector of shape [k], representing the query embedding.\n",
        "    item_embeddings: a matrix of shape [N, k], such that row i is the embedding\n",
        "      of item i.\n",
        "    measure: a string specifying the similarity measure to be used. Can be\n",
        "      either DOT or COSINE.\n",
        "  Returns:\n",
        "    scores: a vector of shape [N], such that scores[i] is the score of item i.\n",
        "  \"\"\"\n",
        "  u = query_embedding\n",
        "  V = item_embeddings\n",
        "  if measure == COSINE:\n",
        "    V = V / np.linalg.norm(V, axis=1, keepdims=True)\n",
        "    u = u / np.linalg.norm(u)\n",
        "  scores = u.dot(V.T)\n",
        "  return scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtq2SncL7FFk",
        "colab_type": "text"
      },
      "source": [
        "Equipped with this function, we can compute recommendations, where the query embedding can be either a user embedding or a movie embedding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIXNd6qg7OLY",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "# @title User recommendations and nearest neighbors (run this cell)\n",
        "def user_recommendations(model, measure=DOT, exclude_rated=False, k=6):\n",
        "  if USER_RATINGS:\n",
        "    scores = compute_scores(\n",
        "        model.embeddings[\"user_id\"][943], model.embeddings[\"movie_id\"], measure)\n",
        "    score_key = measure + ' score'\n",
        "    df = pd.DataFrame({\n",
        "        score_key: list(scores),\n",
        "        'movie_id': movies['item_id'],\n",
        "        'titles': movies['title'],\n",
        "        'genres': movies['all_genres'],\n",
        "    })\n",
        "    if exclude_rated:\n",
        "      # remove movies that are already rated\n",
        "      rated_movies = ratings[ratings.user_id == \"943\"][\"movie_id\"].values\n",
        "      df = df[df.movie_id.apply(lambda movie_id: movie_id not in rated_movies)]\n",
        "    display.display(df.sort_values([score_key], ascending=False).head(k))  \n",
        "\n",
        "def movie_neighbors(model, title_substring, measure=DOT, k=6):\n",
        "  # Search for movie ids that match the given substring.\n",
        "  ids =  movies[movies['title'].str.contains(title_substring)].index.values\n",
        "  titles = movies.iloc[ids]['title'].values\n",
        "  if len(titles) == 0:\n",
        "    raise ValueError(\"Found no movies with title %s\" % title_substring)\n",
        "  print(\"Nearest neighbors of : %s.\" % titles[0])\n",
        "  if len(titles) > 1:\n",
        "    print(\"[Found more than one matching movie. Other candidates: {}]\".format(\n",
        "        \", \".join(titles[1:])))\n",
        "  movie_id = ids[0]\n",
        "  scores = compute_scores(\n",
        "      model.embeddings[\"movie_id\"][movie_id], model.embeddings[\"movie_id\"],\n",
        "      measure)\n",
        "  score_key = measure + ' score'\n",
        "  df = pd.DataFrame({\n",
        "      score_key: list(scores),\n",
        "      'titles': movies['title'],\n",
        "      'genres': movies['all_genres']\n",
        "  })\n",
        "  display.display(df.sort_values([score_key], ascending=False).head(k))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njyV4Fn_8v7O",
        "colab_type": "text"
      },
      "source": [
        "Note: Depending on how the model is initialized, you may observe that some niche movies (ones with few ratings) have a high norm, leading to spurious recommendations. This can happen if the embedding of that movie happens to be initialized with a high norm. Then, because the movie has few ratings, it is infrequently updated, and can keep its high norm. This will be alleviated by using regularization.\n",
        "\n",
        "Try changing the value of the hyper-parameter `init_stddev`. One quantity that can be helpful is that the expected norm of a $d$-dimensional vector with entries $\\sim \\mathcal N(0, \\sigma^2)$ is approximatley $\\sigma \\sqrt d$.\n",
        "\n",
        "How does this affect the embedding norm distribution, and the ranking of the top-norm movies?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGGeCFRK8yxf",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "6f432987-8c7b-4eb5-9f9f-7f0f19c7cfc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        }
      },
      "source": [
        "#@title Solution\n",
        "model_lowinit = build_model(df, embedding_dim=30, init_stddev=0.05)\n",
        "model_lowinit.train(num_iterations=1000, learning_rate=10.)\n",
        "movie_neighbors(model_lowinit, \"Aladdin\", DOT)\n",
        "movie_neighbors(model_lowinit, \"Aladdin\", COSINE)\n",
        "movie_embedding_norm([model, model_lowinit])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-dc45f2100c99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_lowinit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_stddev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel_lowinit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmovie_neighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_lowinit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Aladdin\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDOT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmovie_neighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_lowinit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Aladdin\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCOSINE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmovie_embedding_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_lowinit\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-63-da7af1143f64>\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(ratings, embedding_dim, init_stddev)\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0mtrain_ratings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_ratings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mratings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0;31m# SparseTensor representation of the train and test datasets.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m   \u001b[0mA_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_rating_sparse_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ratings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m   \u001b[0mA_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_rating_sparse_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_ratings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0;31m# Initialize the embeddings using a normal distribution.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-60-d674930d46e4>\u001b[0m in \u001b[0;36mbuild_rating_sparse_tensor\u001b[0;34m(ratings_df)\u001b[0m\n\u001b[1;32m     11\u001b[0m       \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m       \u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m       dense_shape=[users.shape[0], movies.shape[0]])\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'movies' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sXOvOjl9M8m",
        "colab_type": "text"
      },
      "source": [
        "## Embedding visualization\n",
        "Since it is hard to visualize embeddings in a higher-dimensional space (when the embedding dimension $k > 3$), one approach is to project the embeddings to a lower dimensional space. T-SNE (T-distributed Stochastic Neighbor Embedding) is an algorithm that projects the embeddings while attempting to preserve their pariwise distances. It can be useful for visualization, but one should use it with care. For more information on using t-SNE, see [How to Use t-SNE Effectively](https://distill.pub/2016/misread-tsne/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TizxmqQT9UkE",
        "colab_type": "code",
        "outputId": "7c0f4fb3-0682-43ca-858f-9a0763991fee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "tsne_movie_embeddings(model_lowinit)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-606c5457e24b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtsne_movie_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_lowinit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'tsne_movie_embeddings' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGCt0v8o9jOH",
        "colab_type": "text"
      },
      "source": [
        "You can highlight the embeddings of a given genre by clicking on the genres panel (SHIFT+click to select multiple genres).\n",
        "\n",
        "We can observe that the embeddings do not seem to have any notable structure, and the embeddings of a given genre are located all over the embedding space. This confirms the poor quality of the learned embeddings. One of the main reasons, which we will address in the next section, is that we only trained the model on observed pairs, and without regularization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xkv47nAs9oGE",
        "colab_type": "text"
      },
      "source": [
        "# V. Regularization In Matrix Factorization\n",
        "\n",
        "In the previous section, our loss was defined as the mean squared error on the observed part of the rating matrix.  As discussed in the lecture, this can be problematic as the model does not learn how to place the embeddings of irrelevant movies. This phenomenon is known as *folding*.\n",
        "\n",
        "We will add regularization terms that will address this issue. We will use two types of regularization:\n",
        "- Regularization of the model parameters. This is a common $\\ell_2$ regularization term on the embedding matrices, given by $r(U, V) =  \\frac{1}{N} \\sum_i \\|U_i\\|^2 + \\frac{1}{M}\\sum_j \\|V_j\\|^2$.\n",
        "- A global prior that pushes the prediction of any pair towards zero, called the *gravity* term. This is given by $g(U, V) = \\frac{1}{MN} \\sum_{i = 1}^N \\sum_{j = 1}^M \\langle U_i, V_j \\rangle^2$.\n",
        "\n",
        "The total loss is then given by\n",
        "$$\n",
        "\\frac{1}{|\\Omega|}\\sum_{(i, j) \\in \\Omega} (A_{ij} - \\langle U_i, V_j\\rangle)^2 + \\lambda _r r(U, V) + \\lambda_g g(U, V)\n",
        "$$\n",
        "where $\\lambda_r$ and $\\lambda_g$ are two regularization coefficients (hyper-parameters)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQNTBl6G-AeB",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 6: Build a regularized Matrix Factorization model and train it\n",
        "Write a function that builds a regularized model. You are given a function `gravity(U, V)` that computes the gravity term given the two embedding matrices $U$ and $V$.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdhoxoWS-FTS",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "# @title Solution\n",
        "def gravity(U, V):\n",
        "  \"\"\"Creates a gravity loss given two embedding matrices.\"\"\"\n",
        "  return 1. / (U.shape[0].value*V.shape[0].value) * tf.reduce_sum(\n",
        "      tf.matmul(U, U, transpose_a=True) * tf.matmul(V, V, transpose_a=True))\n",
        "\n",
        "def build_regularized_model(\n",
        "    ratings, embedding_dim=3, regularization_coeff=.1, gravity_coeff=1.,\n",
        "    init_stddev=0.1):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    ratings: the DataFrame of movie ratings.\n",
        "    embedding_dim: The dimension of the embedding space.\n",
        "    regularization_coeff: The regularization coefficient lambda.\n",
        "    gravity_coeff: The gravity regularization coefficient lambda_g.\n",
        "  Returns:\n",
        "    A CFModel object that uses a regularized loss.\n",
        "  \"\"\"\n",
        "  # Split the ratings DataFrame into train and test.\n",
        "  train_ratings, test_ratings = split_dataframe(ratings)\n",
        "  # SparseTensor representation of the train and test datasets.\n",
        "  A_train = build_rating_sparse_tensor(train_ratings)\n",
        "  A_test = build_rating_sparse_tensor(test_ratings)\n",
        "  U = tf.Variable(tf.random_normal(\n",
        "      [A_train.dense_shape[0], embedding_dim], stddev=init_stddev))\n",
        "  V = tf.Variable(tf.random_normal(\n",
        "      [A_train.dense_shape[1], embedding_dim], stddev=init_stddev))\n",
        "\n",
        "  error_train = sparse_mean_square_error(A_train, U, V)\n",
        "  error_test = sparse_mean_square_error(A_test, U, V)\n",
        "  gravity_loss = gravity_coeff * gravity(U, V)\n",
        "  regularization_loss = regularization_coeff * (\n",
        "      tf.reduce_sum(U*U)/U.shape[0].value + tf.reduce_sum(V*V)/V.shape[0].value)\n",
        "  total_loss = error_train + regularization_loss + gravity_loss\n",
        "  losses = {\n",
        "      'train_error_observed': error_train,\n",
        "      'test_error_observed': error_test,\n",
        "  }\n",
        "  loss_components = {\n",
        "      'observed_loss': error_train,\n",
        "      'regularization_loss': regularization_loss,\n",
        "      'gravity_loss': gravity_loss,\n",
        "  }\n",
        "  embeddings = {\"user_id\": U, \"movie_id\": V}\n",
        "\n",
        "  return CFModel(embeddings, total_loss, [losses, loss_components])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYgGheqH-NGi",
        "colab_type": "text"
      },
      "source": [
        "It is now time to train the regularized model! You can try different values of the regularization coefficients, and different embedding dimensions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-OIuFlT-Qqx",
        "colab_type": "code",
        "outputId": "f0678ea8-f09c-491b-dfd1-85bf6adc0c62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "source": [
        "reg_model = build_regularized_model(\n",
        "    ratings, regularization_coeff=0.1, gravity_coeff=1.0, embedding_dim=35,\n",
        "    init_stddev=.05)\n",
        "reg_model.train(num_iterations=2000, learning_rate=20.)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-c9ce1f85fa1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m reg_model = build_regularized_model(\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mratings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularization_coeff\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgravity_coeff\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m35\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     init_stddev=.05)\n\u001b[1;32m      4\u001b[0m \u001b[0mreg_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ratings' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKxa1KqR-nRX",
        "colab_type": "text"
      },
      "source": [
        "Observe that adding the regularization terms results in a higher MSE, both on the training and test set. However, as we will see, the quality of the recommendations improves. This highlights a tension between fitting the observed data and minimizing the regularization terms. Fitting the observed data often emphasizes learning high similarity (between items with many interactions), but a good embedding representation also requires learning low similarity (between items with few or no interactions)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IujoWZsH-s6p",
        "colab_type": "text"
      },
      "source": [
        "### Inspect the results\n",
        "Let's see if the results with regularization look better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6NBfhCb-xQe",
        "colab_type": "code",
        "outputId": "416a765e-6ddd-43f4-8142-53954ad60483",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "user_recommendations(reg_model, DOT, exclude_rated=True, k=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-d69e95e48d0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0muser_recommendations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDOT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude_rated\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'reg_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wfe2L1Ye_Y0Q",
        "colab_type": "code",
        "outputId": "f938c8cc-3959-4ca1-d8ab-390403787e0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "# Visualize the embeddings\n",
        "tsne_movie_embeddings(reg_model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-d8a07584d8e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtsne_movie_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'tsne_movie_embeddings' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCBLI2M1_7Fg",
        "colab_type": "text"
      },
      "source": [
        "We should observe that the embeddings have a lot more structure than the unregularized case. Try selecting different genres and observe how they tend to form clusters (for example Horror, Animation and Children).\n",
        "\n",
        "### Conclusion\n",
        "This concludes this section on matrix factorization models. Note that while the scale of the problem is small enough to allow efficient training using SGD, many practical problems need to be trained using more specialized algorithms such as Alternating Least Squares (see [tf.contrib.factorization.WALSMatrixFactorization](https://www.tensorflow.org/api_docs/python/tf/contrib/factorization/WALSMatrixFactorization) for a TF implementation)."
      ]
    }
  ]
}